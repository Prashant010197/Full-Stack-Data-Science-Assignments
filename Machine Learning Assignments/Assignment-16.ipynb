{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "english-offer",
   "metadata": {},
   "source": [
    "# Assignment-16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-identification",
   "metadata": {},
   "source": [
    "## 1. In a linear equation, what is the difference between a dependent variable and an independent variable?\n",
    "\n",
    "Dependent variable is one whose values are dependent on the values of independent variable. In other words, the independent variable's values dictate the values of the dependent variable. In a linear equation, this would mean that the dependent variable would be in a linear relationship with the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-contributor",
   "metadata": {},
   "source": [
    "## 2. What is the concept of simple linear regression? Give a specific example.\n",
    "\n",
    "The main concept is to make a line fit data points of highly correlated pair of variables such that the root mean square error value is the least. This would mean that we need to make sure that the distance between the fitted regression line and the data points(residuals) is least. Several assumptions are made to make simple linear regression work. Linearity, Independence of variables involved, residuals must be normally distributed are assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-holder",
   "metadata": {},
   "source": [
    "## 3. In a linear regression, define the slope.\n",
    "\n",
    "The numerical property of a line that expresses the direction of the line and steepness of the line is called the slope. Slope can be calculated as\n",
    "\n",
    "m=(y2-y1)/(x2-x1)\n",
    "\n",
    "Slope is important in linear regression as it establishes the relationship(negatively linear or positively linear) between feature and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-opposition",
   "metadata": {},
   "source": [
    "## 4. Determine the graph's slope, where the lower point on the line is represented as (3, 2) and the higher point is represented as (2, 2).\n",
    "\n",
    "Slope would be 3/2 or 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-minnesota",
   "metadata": {},
   "source": [
    "## 5. In linear regression, what are the conditions for a positive slope?\n",
    "\n",
    "For a positive slope, the two linearly related variables must have a positive correlation coefficient. (positive slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-separation",
   "metadata": {},
   "source": [
    "## 6. In linear regression, what are the conditions for a negative slope?\n",
    "\n",
    "For a positive slope, the two linearly related variables must have a negative correlation coefficient. (negative slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-frost",
   "metadata": {},
   "source": [
    "## 7. What is multiple linear regression and how does it work?\n",
    "\n",
    "Multiple linear regression is an extension of the concept of simple linear regression where a single dependent variable is linearly correlated and is in linear relationship with multiple independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-injury",
   "metadata": {},
   "source": [
    "## 8. In multiple linear regression, define the number of squares due to error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-intro",
   "metadata": {},
   "source": [
    "## 9. In multiple linear regression, define the number of squares due to regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-holmes",
   "metadata": {},
   "source": [
    "## 10. In a regression equation, what is multicollinearity?\n",
    "\n",
    "Multicollinearity is the property of two or more indepedent variables where the independent variables themselves are in linear relationship among each other. Multicollinearity leads to wrong results in prediction of value of dependent variable in multiple linear regression as change in value of one independent variable is also reflected as change in one or more independent variables.\n",
    "\n",
    "This is the reason why linear regression starts with an assumption of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-noise",
   "metadata": {},
   "source": [
    "## 11. What is heteroskedasticity, and what does it mean?\n",
    "\n",
    "Heteroskedasticity is an assumption used in linear regression. It is simply used to quantify the presence of high changes in residuals for different data points. High changes in residuals across data points in question would lead to poor fitting of a line even when data shows high correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-miniature",
   "metadata": {},
   "source": [
    "## 12. Describe the concept of ridge regression.\n",
    "\n",
    "Logistic regression can use two types of regularization terms along with loss term to reduce overfitting. Using L2 regularization gives us ridge regression. The concept of ridge regression is to penalize the loss function by using the term of sum of square of all feature weights. Regularization term does not make the weights 0 at any time. Ridge regression performs good when more complex algorithmic modelling is a necessity to understand complex data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-posting",
   "metadata": {},
   "source": [
    "## 13. Describe the concept of lasso regression.\n",
    "\n",
    "Using L1 regularization term along with the loss function in logistic regression gives us lasso regression. The concept of lasso regression is to penalize the loss function by using the term of sum of absolute value of all feature weights. Lasso regression performs poorly when more complex algorithmic modelling is a necessity to understand complex data patterns. But it is simpler and easier to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-extraction",
   "metadata": {},
   "source": [
    "## 14. What is polynomial regression and how does it work?\n",
    "\n",
    "Polynomial regression is dependent on underlying relationship of dependent variable to the nth degree of the independent variables. Using quadratic or cubic equation, the same least square technique is used to find the least root mean square error and fit a non linear line over the data which is able to support the non linear nth degree relation between the independent variables and the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-person",
   "metadata": {},
   "source": [
    "## 15. Describe the basis function.\n",
    "\n",
    "A function that expresses the input values to the linear model algorithm as the function of the input is called the basis function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-tokyo",
   "metadata": {},
   "source": [
    "## 16. Describe how logistic regression works.\n",
    "\n",
    "Logistic regression is a binary classifier that starts with the main assumption that the data points can be linearly classified. Logistic regression has an argument minimization equation that needs to be optimized. \n",
    "\n",
    "w*=argmin(sigma(log 1 + exp(-yi.wT.xi)) + lambda.||w||2\n",
    "\n",
    " 1. where yi.wT.xi is the signed distance, positive would mean correct classification and yi.wT.xi negative would mean incorrect classification\n",
    "\n",
    " 2. exp(-yi.wT.xi) is the sigmoid function that tapers off the hyperplane pi so that noise in calculation of signed distance does not affect classification,\n",
    "\n",
    " 3. sigma(log 1 + exp(-yi.wT.xi)) is the loss term,\n",
    "\n",
    " 4. and lambda.||w||2 is the regularization term with L2 regularization. \n",
    "\n",
    "For each data point, wT.xi is calculated as the distance of xi from hyperplane pi. Signed distance is calculated as yi.wT.xi where yi could be 1 or -1.\n",
    "\n",
    "The algorithm tries to minimize the summation of the loss term having signed distance in the form of sigmoid function and the regularization term, while approximating the best hyperplane given the constraints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
